##### K-means clustering #####

# Clear environment
rm(list=ls())
gc()

# Ensure strings are parsed as characters and not factors
options(stringsAsFactors = F)

# Choose the working directly where input files are placed and output files will be written out
setwd(choose.dir())
# Check working directory
getwd()

pckg<-c('qtlcharts','clustertend','factoextra','NbClust','cluster','rgl','clValid')

# Installing missing packages as required
new.packages <- pckg[!(pckg %in% rownames(installed.packages()))]
existing.packages <- pckg[(pckg %in% rownames(installed.packages()))]
lapply(existing.packages,require, character.only = TRUE)
if (length(new.packages)>0)
{install.packages(new.packages)}

# Loading packages as required
existing.packages <- pckg[(pckg %in% rownames(installed.packages()))]
lapply(existing.packages,require, character.only = TRUE)

# Pickup cleaned final file
file_name <- list.files(pattern = "final")

# Read csv file into R as a dataframe
cust_raw <- read.csv(file_name)

# Ensure all missing values are coreced to 0
cust_raw[is.na(cust_raw)] <- 0

# Structure of dataset
str(cust_raw)
View(cust_raw)

# Input only numeric columns for correlation check
cor_ip <- cust_raw[,c(5:length(colnames(cust_raw)))]

str(cor_ip)

# Check correlation plot for multicollinearity
iplotCorr(cor_ip, reorder=TRUE)

# Creating a copy of the correlation input dataframe
cor_ip_2 <- cor_ip


# Removing last 6 months data as it seems redundant
cor_ip_2$last_6_value_per_visit <- NULL
cor_ip_2$last_6_value_per_item <- NULL

iplotCorr(cor_ip_2, reorder=TRUE)

# Looks good, ready for clustering


# Setting seed to ensure results are reproducible
set.seed(10)

# Scaling dataset to ensure standardization of variables with different scales of measurement
# Subtracting mean and dividing by std deviation - z scores
km_ip_scale <- scale(cor_ip_2, scale = T)
View(km_ip_scale)

# Checking Hopkins statistic to assess clustering tendency
# Hopkins test measurs the probability that a given data set is generated by a uniform data distribution 
# Null hypothesis: the data set D is uniformly distributed (i.e., no meaningful clusters)
# In other words, it tests the spatial randomness of the data
# We can conduct the Hopkins Statistic test iteratively, using 0.5 as the threshold to reject the alternative hypothesis 
# That is, if H < 0.5, then it is unlikely that D has statistically significant clusters
hopkins(km_ip_scale, n = 100)

# 0.2322148 - lower than 0.5 meaning data is somewhat uniformly distributed but will proceed as this is an illustrative example
# Link for reference: https://www.datanovia.com/en/lessons/assessing-clustering-tendency/


# Optimum number of clusters comes up to 2 but we will proceed with 5 as ideal output is not required

# Generating the Elbow curve for finding optimum number of clusters
# Elbow curve is a visualization of WCSS (Within-Cluster Sum of Square) on Y-axis for each value of k on X-axis
# WCSS is the sum of squared distance between each point and the centroid in a cluster
# Value of k beyond which the graph starts to move almost parallel to the X-axis is the optimal value of k
# The optimal point is arrived at through visual inspection


fviz_nbclust(km_ip_scale, kmeans, method = "wss", k.max=10) +
  geom_vline(xintercept = 5, linetype = 2) + theme_minimal() + ggtitle("Elbow Method")


# Optimum number of clusters comes up to 5
# Python link for reference: https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/

# Generating Silhouette plot for finding optimum number of clusters #

# Silhouette Score = (b-a)/max(a,b) where..
# a = average intra-cluster distance i.e the average distance between each point within a cluster.
# b = average inter-cluster distance i.e the average distance between all clusters.

# Value varies between -1 and 1 
# 1 = clusters are well apart from each other and clearly distinguished
# 0 = clusters are indifferent, or we can say that the distance between clusters is not significant
# -1 = clusters are assigned in the wrong way
# The optimal number of clusters is the one that maximizes the average silhouette over a range of possible values for k

fviz_nbclust(km_ip_scale, kmeans, method = "silhouette")+
  geom_vline(xintercept = 5, linetype = 3)+
  labs(subtitle = "Silhouette method")

# Python link for reference: https://towardsdatascience.com/silhouette-coefficient-validating-clustering-techniques-e976bb81d10c 
# Optimum number of clusters comes up to 2 but we will proceed with 5

# Running k-means 
km <-  kmeans(x=km_ip_scale, centers=5, iter.max = 10, nstart=100)

# fviz_cluster(km, data = km_ip_scale) 
# plot(cor_ip_2, col =(km$cluster) , main="K-Means result with 5 clusters", pch=20, cex=2)


# The below method provides 30 indices for determining the relevant number of clusters and 
# proposes to users the best clustering scheme from the different results obtained by varying 
# all combinations of number of clusters, distance measures, and clustering methods
# Link for reference: http://www.sthda.com/english/wiki/print.php?id=239

nb <- NbClust(km_ip_scale,  distance = "euclidean", min.nc=2, max.nc=15, method = "kmeans",
              index = "silhouette")
nb$All.index
nb$Best.

## Other methods: 
# Gap statistic 
# Calinski-Harabasz Index 
# Davies-Bouldin Index 
# Bayesian information criterion (BIC) 
# Clustree/Dendrogram 
# R link: https://towardsdatascience.com/10-tips-for-choosing-the-optimal-number-of-clusters-277e93d72d92 
# Python link: https://towardsdatascience.com/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad


# Plotting the clusters in a 2D plot
clusplot(km_ip_scale, km$cluster, color=TRUE, shade=TRUE, 
         labels=2, lines=1)



# Assigning the cluster membership to the data points
km_ip_scale_km <- cbind(km_ip_scale,km$cluster)
View(km_ip_scale_km)
colnames(km_ip_scale_km)[5] <- "clust"

# Converting matrix to dataframe
km_ip_scale_km <- as.data.frame(km_ip_scale_km)


# Plotting a 3D plot of dimensions with clusters being different colors
# Choose 3 variables as desired and input the same in the below function
plot3d(km_ip_scale_km$age, km_ip_scale_km$months_since_last_purch, km_ip_scale_km$reg_value_per_visit,
       col=km_ip_scale_km$clust,size=5)


# Generating output dataset
kmeans_op <- cbind(cust_raw$id,cor_ip_2,km$cluster)


# Writing output dataset to csv
write.csv(kmeans_op,"kmeans_op.csv", row.names = F)

### Additonal clustering validation ###
# Loading package for clustering validation
if(!require(clValid)) install.packages("clValid")

# Internal Validation options
clmethods <- c("hierarchical","kmeans","pam")

# Generating a sample of 500 observations from the select dataset for validation
km_ip_samp <- cor_ip_2[sample(nrow(cor_ip_2),500),]

# Scaling the sample dataset
km_ip_scale_samp <- scale(km_ip_samp, scale = T)

# Running the internal validation
internval <- clValid(km_ip_scale_samp, 
                     nClust = 2:7, clMethods = clmethods, validation = "internal")

# Summary of internal validation methods
summary(internval)
# Connectivity - what extent items are placed in the same cluster as their nearest neighbors 
# in the data space. It has a value between 0 and infinity and should be minimized

# Average Silhouette width - It lies between -1 (poorly clustered observations) to 1 (well clustered observations). 
# It should be maximized

# Dunn index - It is the ratio between the smallest distance between observations 
# not in the same cluster to the largest intra-cluster distance. 
# It has a value between 0 and infinity and should be maximized

optimalScores(internval)